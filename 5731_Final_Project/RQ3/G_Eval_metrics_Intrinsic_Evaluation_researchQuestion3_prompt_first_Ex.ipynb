{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LavanyaPobbathi/Lavanya_INFO5731_Fall2023/blob/main/5731_Final_Project/RQ3/G_Eval_metrics_Intrinsic_Evaluation_researchQuestion3_prompt_first_Ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzMdRyQ7RIBd",
        "outputId": "cc0d8046-e496-4611-be26-e4289c21b0f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install rouge --quiet\n",
        "!pip install bert_score --quiet\n",
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4oYpBRWU-jd",
        "outputId": "27f26e93-e84a-40d1-b462-f4d0955717b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First Experiment"
      ],
      "metadata": {
        "id": "nq3mg6PFbbsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Python Implementation of the ROUGE Metric\n",
        "from rouge import Rouge\n",
        "\n",
        "# BERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences by cosine similarity.\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "openai.api_key = os.environ.get(\"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\")"
      ],
      "metadata": {
        "id": "bGUohAuLRMqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_Prompt1 = \"\"\"\n",
        "\n",
        "The patent outlines an immersive viewing system, especially designed for videoconferencing, featuring a movement detector and an active screen displayer. The movement detector, employing sensors and cameras, monitors user proximity to the display through a distance sensor. Simultaneously, the active screen displayer dynamically adjusts the display view based on changes in user distance, delivering an engaging visual experience. The invention targets multiple users, introducing key claims involving sensors for detecting movements and determining distances for each user. The processor assigns priorities to these movements, facilitating the updating of displayed scenes on the display. Noteworthy features include the ability to zoom in/out, alter viewing directions, and assign priorities based on user input. The method claims emphasize detecting movements, determining distances, assigning priorities, and updating scenes, encompassing applications in various scenarios, such as video gaming and architectural walkthroughs. Overall, the patent introduces an innovative, user-centric approach to immersive displays with broad applications.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FhdlNIKqRUsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_Prompt2 = \"\"\"\n",
        "\n",
        "This Google patent introduces an immersive viewer system designed primarily for interactive video displays, particularly in videoconferencing terminals. The apparatus comprises a movement detector for monitoring user movement and a distance sensor within it to measure the user's distance from the display. The system operates independently, with an active screen displayer for scene navigation and dynamic display updates based on changes in the user's distance. The innovation lies in the dynamic adjustment of the display concerning user movement and distance, ultimately aiming to provide an immersive viewing experience with improved interaction based on user proximity. The patent focuses on integrating distance sensing for dynamic display adjustments, envisioning applications in virtual reality, gaming, and interactive displays.\n",
        "The patent's claims delineate distinct functionalities, including dual user detection, zooming capabilities, alteration of viewing direction, user input-based prioritization, distance-based prioritization, camera field of view adjustment, multi-axis detection, and application in video games. These claims collectively contribute to the system's ability to provide an immersive viewing experience by updating the displayed scene in real-time based on user movements and distances.\n",
        "The detailed description emphasizes the technical aspects, highlighting the role of the movement detector with disengaged sensors, like cameras, and the active screen displayer. This non-tactile approach enables users to interact without additional equipment, enhancing the system's versatility for videoconferencing, architectural walkthroughs, interactive displays, and video gaming. The core innovation of real-time display updating enhances user engagement, allowing for remote collaboration without cumbersome equipment.\n",
        "In summary, this patent introduces a comprehensive immersive viewer system that leverages advanced sensor technology to enable dynamic adjustments of the display, providing an enhanced and interactive user experience across various applications and industries.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dUCb7vRaSXbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_Prompt3 = \"\"\"\n",
        "\n",
        "This Google patent application describes a technology designed to enhance interactive video displays, particularly in videoconferencing settings, with a focus on creating more immersive and natural user experiences. The primary objective of this innovation is to enable users to control and navigate displayed scenes on a screen through their movements, without requiring physical sensors or controls on their bodies.\n",
        "\n",
        "Key innovations and concepts outlined in this patent application include:\n",
        "\n",
        "Movement Monitoring: The system employs sensors, including cameras, to monitor users' horizontal and vertical movements in front of the display.\n",
        "\n",
        "Camera Adjustments: The technology adjusts the camera's field of view based on detected user movements, ensuring users remain within the camera's perspective.\n",
        "\n",
        "Display Direction: The system also alters the viewing direction of the display to match user movements, enhancing the sense of immersion.\n",
        "\n",
        "Scene Navigation: Users can navigate displayed scenes by moving closer or farther from the screen, with corresponding zooming in or out. This navigation applies to various types of scenes, including remote, virtual, recorded, or video game scenes.\n",
        "\n",
        "No Physical Sensors or Controls: A significant innovation is that users don't need to wear any additional sensors or manipulate physical controls to achieve these interactions. The system relies solely on detected user movements.\n",
        "\n",
        "Remote Collaboration: The patent application envisions applications in remote collaboration and conferencing, allowing users to virtually move around remote spaces as if they were physically present.\n",
        "\n",
        "Diverse Applications: Beyond videoconferencing, the technology has potential applications in architectural walkthroughs, interactive corporate displays, video gaming, and managing virtual desktops that extend beyond the display's visible area.\n",
        "\n",
        "In summary, this Google patent application introduces a technology that aims to create more immersive and natural interactions in video displays by allowing users to control and navigate scenes using their movements in front of the screen, without the need for additional equipment or sensors. It has broad potential applications in various domains, enhancing user experiences in interactive displays and video communication.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rGjBlWQKYnIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_Prompt4 = \"\"\"\n",
        "Abstract:\n",
        "The key objective of the abstract is to describe an apparatus and method for providing an immersive viewing experience on a display by monitoring a user's movement using a distance sensor and dynamically updating the displayed scene based on changes in the user's distance from the display. This technology aims to enhance the user's interaction with and viewing of content on the screen by adjusting the displayed scene in response to their movements.\n",
        "\n",
        "Claims:\n",
        "\n",
        "The apparatus detects movements of multiple users and their respective distances from a display, assigning different priorities to the detected movements, and updating the displayed scene based on these priorities.\n",
        "The apparatus, in addition to Claim 1, updates the displayed scene by zooming in or out based on the assigned priorities.\n",
        "The apparatus, in addition to Claim 1, updates the displayed scene by altering the viewing direction of the scene based on the assigned priorities.\n",
        "The apparatus assigns different priorities to the detected movements of users based on user input.\n",
        "The apparatus assigns different priorities to the detected movements of users based on the determined distances of users from the display.\n",
        "The apparatus updates the displayed scene by changing the field of view of a camera capturing the scene using the determined distances and the assigned priorities.\n",
        "The sensors detect movements and determine distances to users in different axes of direction.\n",
        "The apparatus updates the displayed scene in a video game using the determined distances and assigned priorities.\n",
        "The method detects movements of multiple users, determines their distances from a display, assigns different priorities to their movements, and updates the displayed scene based on these priorities.\n",
        "The method, in addition to Claim 9, updates the displayed scene by zooming in or out based on the assigned priorities.\n",
        "The method, in addition to Claim 9, updates the displayed scene by altering the viewing direction of the scene based on the assigned priorities.\n",
        "The method assigns different priorities to the detected movements of users based on user input.\n",
        "The method assigns different priorities to the detected movements of users based on the determined distances of users from the display.\n",
        "The method updates the displayed scene by changing the field of view of a camera capturing the scene using the determined distances and the assigned priorities.\n",
        "The method detects movements and determines distances to users in different axes of direction.\n",
        "The method updates the displayed scene in a video game using the determined distances and assigned priorities.\n",
        "Description Summary:\n",
        "\n",
        "The patent application relates to interactive video displays, particularly for videoconferencing terminals.\n",
        "The goal is to improve the user experience in video communication by enabling natural and immersive interactions.\n",
        "The system includes a local display, a movement detector with sensors (cameras and distance sensors), and optionally, a remote display and camera for videoconferencing.\n",
        "The movement detector monitors the user's movements, including horizontal and vertical shifts and changes in distance from the display.\n",
        "Based on the detected movements, the system dynamically adjusts both the camera's field of view and the viewing direction of the display.\n",
        "Users can control the displayed content and camera views through natural movements, such as zooming in or out by moving closer to or away from the display.\n",
        "The system eliminates the need for physical sensors or controls, providing a more immersive and interactive video communication experience.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sFn5khu5YnR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"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\"  # Replace with your actual API key\n",
        ")\n",
        "\n",
        "# Define the evaluation prompt template for Google patent summaries.\n",
        "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You will be given one summary written for a Google patent. Your task is to rate the summary on one metric.\n",
        "Please make sure you read and understand these instructions very carefully.\n",
        "Please keep this document open while reviewing, and refer to it as needed.\n",
        "\n",
        "Evaluation Criteria:\n",
        "\n",
        "{criteria}\n",
        "\n",
        "Evaluation Steps:\n",
        "\n",
        "{steps}\n",
        "\n",
        "Example:\n",
        "\n",
        "Summary:\n",
        "\n",
        "{summary}\n",
        "\n",
        "Evaluation Form (scores ONLY):\n",
        "\n",
        "- {metric_name}\n",
        "\"\"\"\n",
        "\n",
        "# Metric 1: Relevance\n",
        "\n",
        "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
        "Relevance(1-5) - selection of important content from the source. \\\n",
        "The summary should include only important information from the source document. \\\n",
        "Annotators were instructed to penalize summaries which contained redundancies and excess information.\n",
        "\"\"\"\n",
        "\n",
        "RELEVANCY_SCORE_STEPS = \"\"\"\n",
        "1. Read the summary and the source document carefully.\n",
        "2. Compare the summary to the source document and identify the main points of the article.\n",
        "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
        "4. Assign a relevance score from 1 to 5.\n",
        "\"\"\"\n",
        "\n",
        "# Metric 2: Coherence\n",
        "\n",
        "COHERENCE_SCORE_CRITERIA = \"\"\"\n",
        "Coherence(1-5) - the collective quality of all sentences. \\\n",
        "We align this dimension with the DUC quality question of structure and coherence \\\n",
        "whereby \"the summary should be well-structured and well-organized. \\\n",
        "The summary should not just be a heap of related information, but should build from sentence to a\\\n",
        "coherent body of information about a topic.\"\n",
        "\"\"\"\n",
        "\n",
        "COHERENCE_SCORE_STEPS = \"\"\"\n",
        "1. Read the Google patent carefully and identify the main topic and key points.\n",
        "2. Read the summary and compare it to the Google patent. Check if the summary covers the main topic and key points of the Google patent,\n",
        "and if it presents them in a clear and logical order.\n",
        "3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
        "\"\"\"\n",
        "\n",
        "# Metric 3: Consistency\n",
        "\n",
        "CONSISTENCY_SCORE_CRITERIA = \"\"\"\n",
        "Consistency(1-5) - the factual alignment between the summary and the summarized source. \\\n",
        "A factually consistent summary contains only statements that are entailed by the source document. \\\n",
        "Annotators were also asked to penalize summaries that contained hallucinated facts.\n",
        "\"\"\"\n",
        "\n",
        "CONSISTENCY_SCORE_STEPS = \"\"\"\n",
        "1. Read the Google patent carefully and identify the main facts and details it presents.\n",
        "2. Read the summary and compare it to the Google patent. Check if the summary contains any factual errors that are not supported by the Google patent.\n",
        "3. Assign a score for consistency based on the Evaluation Criteria.\n",
        "\"\"\"\n",
        "\n",
        "# Metric 4: Fluency\n",
        "\n",
        "FLUENCY_SCORE_CRITERIA = \"\"\"\n",
        "Fluency(1-5): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n",
        "1: Poor. The summary has many errors that make it hard to understand or sound unnatural.\n",
        "2: Fair. The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.\n",
        "3: Good. The summary has few or no errors and is easy to read and follow.\n",
        "\"\"\"\n",
        "\n",
        "FLUENCY_SCORE_STEPS = \"\"\"\n",
        "Read the summary and evaluate its fluency based on the given criteria. Assign a fluency score from 1 to 5.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to get G-Eval scores using GPT-4.\n",
        "def get_geval_score(criteria, steps, summary, metric_name):\n",
        "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
        "        criteria=criteria,\n",
        "        steps=steps,\n",
        "        metric_name=metric_name,\n",
        "        summary=summary,\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "        max_tokens=5,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "evaluation_metrics = {\n",
        "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
        "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
        "    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n",
        "    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n",
        "}\n",
        "\n",
        "# Define the summaries for each image.\n",
        "summaries = {\n",
        "    \"GPT3.5 Prompt1\": summary_Prompt1,  # replace with your first summary\n",
        "    \"GPT3.5 Prompt2\": summary_Prompt2,  # replace with your second summary\n",
        "    \"GPT3.5 Prompt3\": summary_Prompt3,  # replace with your third summary\n",
        "    \"GPT3.5 Prompt4\": summary_Prompt4,  # replace with your fourth summary\n",
        "}\n",
        "\n",
        "# Initialize data for the DataFrame.\n",
        "data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n",
        "\n",
        "# Evaluate each summary for each metric.\n",
        "for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
        "    for summ_type, summary in summaries.items():\n",
        "        data[\"Evaluation Type\"].append(eval_type)\n",
        "        data[\"Summary Type\"].append(summ_type)\n",
        "        result = get_geval_score(criteria, steps, summary, eval_type)\n",
        "        score_num = int(result.strip())\n",
        "        data[\"Score\"].append(score_num)\n",
        "\n",
        "# Create a pivot table and display it.\n",
        "pivot_df = pd.DataFrame(data, index=None).pivot(\n",
        "    index=\"Evaluation Type\", columns=\"Summary Type\", values=\"Score\"\n",
        ")\n",
        "\n",
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return [\n",
        "        \"background-color: lightgreen\" if v else \"background-color: white\"\n",
        "        for v in is_max\n",
        "    ]\n",
        "\n",
        "styled_pivot_df = pivot_df.style.apply(highlight_max, axis=1)\n",
        "display(styled_pivot_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mvcgdroyRIWb",
        "outputId": "27708f8e-02c9-45f0-808d-c080c966e097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7e20deb49c30>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_d941d_row0_col0, #T_d941d_row0_col1, #T_d941d_row0_col2, #T_d941d_row1_col0, #T_d941d_row1_col1, #T_d941d_row2_col0, #T_d941d_row2_col1, #T_d941d_row2_col2, #T_d941d_row2_col3, #T_d941d_row3_col1, #T_d941d_row3_col2 {\n",
              "  background-color: lightgreen;\n",
              "}\n",
              "#T_d941d_row0_col3, #T_d941d_row1_col2, #T_d941d_row1_col3, #T_d941d_row3_col0, #T_d941d_row3_col3 {\n",
              "  background-color: white;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_d941d\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Summary Type</th>\n",
              "      <th id=\"T_d941d_level0_col0\" class=\"col_heading level0 col0\" >GPT3.5 Prompt1</th>\n",
              "      <th id=\"T_d941d_level0_col1\" class=\"col_heading level0 col1\" >GPT3.5 Prompt2</th>\n",
              "      <th id=\"T_d941d_level0_col2\" class=\"col_heading level0 col2\" >GPT3.5 Prompt3</th>\n",
              "      <th id=\"T_d941d_level0_col3\" class=\"col_heading level0 col3\" >GPT3.5 Prompt4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Evaluation Type</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_d941d_level0_row0\" class=\"row_heading level0 row0\" >Coherence</th>\n",
              "      <td id=\"T_d941d_row0_col0\" class=\"data row0 col0\" >5</td>\n",
              "      <td id=\"T_d941d_row0_col1\" class=\"data row0 col1\" >5</td>\n",
              "      <td id=\"T_d941d_row0_col2\" class=\"data row0 col2\" >5</td>\n",
              "      <td id=\"T_d941d_row0_col3\" class=\"data row0 col3\" >4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d941d_level0_row1\" class=\"row_heading level0 row1\" >Consistency</th>\n",
              "      <td id=\"T_d941d_row1_col0\" class=\"data row1 col0\" >5</td>\n",
              "      <td id=\"T_d941d_row1_col1\" class=\"data row1 col1\" >5</td>\n",
              "      <td id=\"T_d941d_row1_col2\" class=\"data row1 col2\" >4</td>\n",
              "      <td id=\"T_d941d_row1_col3\" class=\"data row1 col3\" >4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d941d_level0_row2\" class=\"row_heading level0 row2\" >Fluency</th>\n",
              "      <td id=\"T_d941d_row2_col0\" class=\"data row2 col0\" >4</td>\n",
              "      <td id=\"T_d941d_row2_col1\" class=\"data row2 col1\" >4</td>\n",
              "      <td id=\"T_d941d_row2_col2\" class=\"data row2 col2\" >4</td>\n",
              "      <td id=\"T_d941d_row2_col3\" class=\"data row2 col3\" >4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d941d_level0_row3\" class=\"row_heading level0 row3\" >Relevance</th>\n",
              "      <td id=\"T_d941d_row3_col0\" class=\"data row3 col0\" >4</td>\n",
              "      <td id=\"T_d941d_row3_col1\" class=\"data row3 col1\" >5</td>\n",
              "      <td id=\"T_d941d_row3_col2\" class=\"data row3 col2\" >5</td>\n",
              "      <td id=\"T_d941d_row3_col3\" class=\"data row3 col3\" >4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kK5fIhPpWkEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M5DnfKYq4PDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0pfDo5S4PHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"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\"  # Replace with your actual API key\n",
        ")\n",
        "\n",
        "def evaluate_text(text):\n",
        "    criteria = [\"Fluency\", \"Coherence\", \"Informativeness\", \"Abstraction\", \"Consistency\"]\n",
        "    scores = {}\n",
        "    for criterion in criteria:\n",
        "        prompt = f\"Please evaluate the following text based on {criterion} on a scale from 0 to 10 (provide only score):\\n\\n{text}\"\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0,\n",
        "        )\n",
        "        score = response.choices[0].message.content.strip()\n",
        "        scores[criterion] = score\n",
        "    return scores\n",
        "\n",
        "prompt1 = \"\"\"\n",
        "The patent outlines an immersive viewing system, especially designed for videoconferencing, featuring a movement detector and an active screen displayer. The movement detector, employing sensors and cameras, monitors user proximity to the display through a distance sensor. Simultaneously, the active screen displayer dynamically adjusts the display view based on changes in user distance, delivering an engaging visual experience. The invention targets multiple users, introducing key claims involving sensors for detecting movements and determining distances for each user. The processor assigns priorities to these movements, facilitating the updating of displayed scenes on the display. Noteworthy features include the ability to zoom in/out, alter viewing directions, and assign priorities based on user input. The method claims emphasize detecting movements, determining distances, assigning priorities, and updating scenes, encompassing applications in various scenarios, such as video gaming and architectural walkthroughs. Overall, the patent introduces an innovative, user-centric approach to immersive displays with broad applications.\n",
        "\"\"\"\n",
        "scores = evaluate_text(prompt1)\n",
        "print(\"GPT3.5 Intrinsic Evaluation Scores for Final Summary prompt1\")\n",
        "for criterion, score in scores.items():\n",
        "    print(f\"{criterion}: {score}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "prompt2 = \"\"\"\n",
        "This Google patent introduces an immersive viewer system designed primarily for interactive video displays, particularly in videoconferencing terminals. The apparatus comprises a movement detector for monitoring user movement and a distance sensor within it to measure the user's distance from the display. The system operates independently, with an active screen displayer for scene navigation and dynamic display updates based on changes in the user's distance. The innovation lies in the dynamic adjustment of the display concerning user movement and distance, ultimately aiming to provide an immersive viewing experience with improved interaction based on user proximity. The patent focuses on integrating distance sensing for dynamic display adjustments, envisioning applications in virtual reality, gaming, and interactive displays.\n",
        "The patent's claims delineate distinct functionalities, including dual user detection, zooming capabilities, alteration of viewing direction, user input-based prioritization, distance-based prioritization, camera field of view adjustment, multi-axis detection, and application in video games. These claims collectively contribute to the system's ability to provide an immersive viewing experience by updating the displayed scene in real-time based on user movements and distances.\n",
        "The detailed description emphasizes the technical aspects, highlighting the role of the movement detector with disengaged sensors, like cameras, and the active screen displayer. This non-tactile approach enables users to interact without additional equipment, enhancing the system's versatility for videoconferencing, architectural walkthroughs, interactive displays, and video gaming. The core innovation of real-time display updating enhances user engagement, allowing for remote collaboration without cumbersome equipment.\n",
        "In summary, this patent introduces a comprehensive immersive viewer system that leverages advanced sensor technology to enable dynamic adjustments of the display, providing an enhanced and interactive user experience across various applications and industries.\n",
        "\n",
        "\"\"\"\n",
        "scores = evaluate_text(prompt2)\n",
        "print(\"GPT3.5 Intrinsic Evaluation Scores for Final Summary prompt2\")\n",
        "for criterion, score in scores.items():\n",
        "    print(f\"{criterion}: {score}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "prompt3 = \"\"\"\n",
        "This Google patent application describes a technology designed to enhance interactive video displays, particularly in videoconferencing settings, with a focus on creating more immersive and natural user experiences. The primary objective of this innovation is to enable users to control and navigate displayed scenes on a screen through their movements, without requiring physical sensors or controls on their bodies.\n",
        "\n",
        "Key innovations and concepts outlined in this patent application include:\n",
        "\n",
        "Movement Monitoring: The system employs sensors, including cameras, to monitor users' horizontal and vertical movements in front of the display.\n",
        "\n",
        "Camera Adjustments: The technology adjusts the camera's field of view based on detected user movements, ensuring users remain within the camera's perspective.\n",
        "\n",
        "Display Direction: The system also alters the viewing direction of the display to match user movements, enhancing the sense of immersion.\n",
        "\n",
        "Scene Navigation: Users can navigate displayed scenes by moving closer or farther from the screen, with corresponding zooming in or out. This navigation applies to various types of scenes, including remote, virtual, recorded, or video game scenes.\n",
        "\n",
        "No Physical Sensors or Controls: A significant innovation is that users don't need to wear any additional sensors or manipulate physical controls to achieve these interactions. The system relies solely on detected user movements.\n",
        "\n",
        "Remote Collaboration: The patent application envisions applications in remote collaboration and conferencing, allowing users to virtually move around remote spaces as if they were physically present.\n",
        "\n",
        "Diverse Applications: Beyond videoconferencing, the technology has potential applications in architectural walkthroughs, interactive corporate displays, video gaming, and managing virtual desktops that extend beyond the display's visible area.\n",
        "\n",
        "In summary, this Google patent application introduces a technology that aims to create more immersive and natural interactions in video displays by allowing users to control and navigate scenes using their movements in front of the screen, without the need for additional equipment or sensors. It has broad potential applications in various domains, enhancing user experiences in interactive displays and video communication.\n",
        "\"\"\"\n",
        "scores = evaluate_text(prompt3)\n",
        "print(\"GPT3.5 Intrinsic Evaluation Scores for Final Summary prompt3\")\n",
        "for criterion, score in scores.items():\n",
        "    print(f\"{criterion}: {score}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "prompt4 = \"\"\"\n",
        "\n",
        "Abstract:\n",
        "The key objective of the abstract is to describe an apparatus and method for providing an immersive viewing experience on a display by monitoring a user's movement using a distance sensor and dynamically updating the displayed scene based on changes in the user's distance from the display. This technology aims to enhance the user's interaction with and viewing of content on the screen by adjusting the displayed scene in response to their movements.\n",
        "\n",
        "Claims:\n",
        "\n",
        "The apparatus detects movements of multiple users and their respective distances from a display, assigning different priorities to the detected movements, and updating the displayed scene based on these priorities.\n",
        "The apparatus, in addition to Claim 1, updates the displayed scene by zooming in or out based on the assigned priorities.\n",
        "The apparatus, in addition to Claim 1, updates the displayed scene by altering the viewing direction of the scene based on the assigned priorities.\n",
        "The apparatus assigns different priorities to the detected movements of users based on user input.\n",
        "The apparatus assigns different priorities to the detected movements of users based on the determined distances of users from the display.\n",
        "The apparatus updates the displayed scene by changing the field of view of a camera capturing the scene using the determined distances and the assigned priorities.\n",
        "The sensors detect movements and determine distances to users in different axes of direction.\n",
        "The apparatus updates the displayed scene in a video game using the determined distances and assigned priorities.\n",
        "The method detects movements of multiple users, determines their distances from a display, assigns different priorities to their movements, and updates the displayed scene based on these priorities.\n",
        "The method, in addition to Claim 9, updates the displayed scene by zooming in or out based on the assigned priorities.\n",
        "The method, in addition to Claim 9, updates the displayed scene by altering the viewing direction of the scene based on the assigned priorities.\n",
        "The method assigns different priorities to the detected movements of users based on user input.\n",
        "The method assigns different priorities to the detected movements of users based on the determined distances of users from the display.\n",
        "The method updates the displayed scene by changing the field of view of a camera capturing the scene using the determined distances and the assigned priorities.\n",
        "The method detects movements and determines distances to users in different axes of direction.\n",
        "The method updates the displayed scene in a video game using the determined distances and assigned priorities.\n",
        "Description Summary:\n",
        "\n",
        "The patent application relates to interactive video displays, particularly for videoconferencing terminals.\n",
        "The goal is to improve the user experience in video communication by enabling natural and immersive interactions.\n",
        "The system includes a local display, a movement detector with sensors (cameras and distance sensors), and optionally, a remote display and camera for videoconferencing.\n",
        "The movement detector monitors the user's movements, including horizontal and vertical shifts and changes in distance from the display.\n",
        "Based on the detected movements, the system dynamically adjusts both the camera's field of view and the viewing direction of the display.\n",
        "Users can control the displayed content and camera views through natural movements, such as zooming in or out by moving closer to or away from the display.\n",
        "The system eliminates the need for physical sensors or controls, providing a more immersive and interactive video communication experience.\n",
        "\"\"\"\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "scores = evaluate_text(prompt4)\n",
        "print(\"GPT3.5 Intrinsic Evaluation Scores for Final Summary prompt4\")\n",
        "for criterion, score in scores.items():\n",
        "    print(f\"{criterion}: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6sCnfZQ4PLK",
        "outputId": "3e1cc101-009b-47d1-dee4-dd41b5a92d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT3.5 Intrinsic Evaluation Scores for Final Summary prompt1\n",
            "Fluency: 8.5\n",
            "Coherence: 8.5\n",
            "Informativeness: 9.5\n",
            "Abstraction: 8.5\n",
            "Consistency: 8.5\n",
            "\n",
            "\n",
            "\n",
            "GPT3.5 Intrinsic Evaluation Scores for Final Summary prompt2\n",
            "Fluency: Fluency score: 8.5\n",
            "Coherence: 8.5\n",
            "Informativeness: 9.5\n",
            "Abstraction: 8.5\n",
            "Consistency: 8.5\n",
            "\n",
            "\n",
            "\n",
            "GPT3.5 Intrinsic Evaluation Scores for Final Summary prompt3\n",
            "Fluency: Fluency score: 9.5\n",
            "Coherence: 8.5\n",
            "Informativeness: 9.5\n",
            "Abstraction: 8.5\n",
            "Consistency: 8.5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "GPT3.5 Intrinsic Evaluation Scores for Final Summary prompt4\n",
            "Fluency: Fluency score: 8.5\n",
            "Coherence: 8.5\n",
            "Informativeness: 8\n",
            "Abstraction: 8.5\n",
            "Consistency: 8.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1VfSn1xbiwM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}